{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multilabel-classification Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model development with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF Model for Contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_contents = pd.read_csv(r\"..\\Nancy PsyData\\nancy_contents.csv\")\n",
        "df_determinants = pd.read_csv(r\"..\\Nancy PsyData\\nancy_determinants.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Réponse (French)', 'Answer (English)', 'Contenu', '(A)', '(AD)', '(H)',\n",
              "       '(HD)', 'A', 'ABS', 'AD', 'ALIM', 'ANAT', 'ARCH', 'ART', 'BOT', 'ELEM',\n",
              "       'FRAG', 'GÉO', 'H', 'HD', 'MQ', 'NAT', 'OBJ', 'PAYS', 'RADIO', 'SC',\n",
              "       'SCÈNE', 'SEX', 'SG', 'VÊT'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_contents.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Réponse (French)', 'Answer (English)', 'Déterminant', 'C', 'C'', 'C'F',\n",
              "       'CF', 'CF'', 'CLOB', 'CLOBF', 'E', 'EF', 'F', 'FC', 'FC'', 'FCLOB',\n",
              "       'FE', 'K', 'KAN', 'KOB', 'KP'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_determinants.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "X = df_contents['Answer (English)'].to_numpy()\n",
        "\n",
        "y_contents = df_contents[['(A)', '(AD)', '(H)',\n",
        "       '(HD)', 'A', 'ABS', 'AD', 'ALIM', 'ANAT', 'ARCH', 'ART', 'BOT', 'ELEM',\n",
        "       'FRAG', 'GÉO', 'H', 'HD', 'MQ', 'NAT', 'OBJ', 'PAYS', 'RADIO', 'SC',\n",
        "       'SCÈNE', 'SEX', 'SG', 'VÊT']].to_numpy()\n",
        "\n",
        "y_determinants = df_determinants[['C', 'C\\'', 'C\\'F',\n",
        "       'CF', 'CF\\'', 'CLOB', 'CLOBF', 'E', 'EF', 'F', 'FC', 'FC\\'', 'FCLOB',\n",
        "       'FE', 'K', 'KAN', 'KOB', 'KP']].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = [KNeighborsClassifier(), LogisticRegression(random_state=42, solver = \"sag\"), SVC(), \n",
        "          RandomForestClassifier(random_state=42), SGDClassifier(random_state=42), GradientBoostingClassifier(random_state=42)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNeighborsClassifier() 0.10526315789473684 0.19607843137254902\n",
            "LogisticRegression(random_state=42, solver='sag') 0.0 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVC() 0.0 0.0\n",
            "RandomForestClassifier(random_state=42) 0.05263157894736842 0.07999999999999999\n",
            "SGDClassifier(random_state=42) 0.18421052631578946 0.3428571428571429\n",
            "GradientBoostingClassifier(random_state=42) 0.2894736842105263 0.4788732394366198\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y_contents, test_size = 0.10, random_state = 42)\n",
        "\n",
        "for clf in models:\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "                    ('text_tfidf', TfidfVectorizer(min_df = 2, max_df = 0.3, ngram_range = (1,3))),\n",
        "                    ('clf', OneVsRestClassifier(clf))\n",
        "                ])\n",
        "\n",
        "    model = pipeline.fit(X_train, y_train)\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions, average=\"micro\")\n",
        "\n",
        "    print(clf, accuracy, f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         4\n",
            "           1       0.00      0.00      0.00         0\n",
            "           2       0.00      0.00      0.00         4\n",
            "           3       0.00      0.00      0.00         1\n",
            "           4       0.67      0.20      0.31        10\n",
            "           5       0.00      0.00      0.00         1\n",
            "           6       0.00      0.00      0.00         4\n",
            "           7       0.00      0.00      0.00         1\n",
            "           8       0.00      0.00      0.00         0\n",
            "           9       0.00      0.00      0.00         0\n",
            "          10       0.00      0.00      0.00         0\n",
            "          11       0.00      0.00      0.00         2\n",
            "          12       0.00      0.00      0.00         0\n",
            "          13       0.00      0.00      0.00         3\n",
            "          14       0.00      0.00      0.00         0\n",
            "          15       0.00      0.00      0.00         2\n",
            "          16       0.00      0.00      0.00         4\n",
            "          17       0.00      0.00      0.00         1\n",
            "          18       0.00      0.00      0.00         0\n",
            "          19       0.00      0.00      0.00         4\n",
            "          20       0.00      0.00      0.00         1\n",
            "          21       0.00      0.00      0.00         0\n",
            "          22       0.00      0.00      0.00         0\n",
            "          23       0.00      0.00      0.00         0\n",
            "          24       0.00      0.00      0.00         0\n",
            "          25       0.00      0.00      0.00         2\n",
            "          26       0.00      0.00      0.00         1\n",
            "\n",
            "   micro avg       0.40      0.04      0.08        45\n",
            "   macro avg       0.02      0.01      0.01        45\n",
            "weighted avg       0.15      0.04      0.07        45\n",
            " samples avg       0.05      0.05      0.05        45\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "class_report = classification_report(y_test, predictions)\n",
        "print(class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the classification report it is evident that the imbalanced dataset is causing problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "pickle.dump(model, open(r\"..\\Models\\Contents\\pipeline_contents_One-Many_V4-11-05.sav\", 'wb')) \n",
        "\n",
        "# will save the RF model since it was at the end of the list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF Model for Determinants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our models are not doing too good, maybe we could \"cascade\" the classifiers if the predictions of a previous classifier were good.\n",
        "For example, computing content labels first and then using the content label prediction to inform the determinant prediction model\n",
        "IFF one of the models was good "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNeighborsClassifier() 0.2631578947368421 0.3278688524590164\n",
            "LogisticRegression(random_state=42, solver='sag') 0.21052631578947367 0.3018867924528302\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVC() 0.18421052631578946 0.2745098039215686\n",
            "RandomForestClassifier(random_state=42) 0.21052631578947367 0.2857142857142857\n",
            "SGDClassifier(random_state=42) 0.2894736842105263 0.41791044776119407\n",
            "GradientBoostingClassifier(random_state=42) 0.21052631578947367 0.2608695652173913\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y_determinants, test_size = 0.10, random_state = 42)\n",
        "\n",
        "for clf in models:\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "                    ('text_tfidf', TfidfVectorizer(min_df = 2, max_df = 0.3, ngram_range = (1,3))),\n",
        "                    ('clf', OneVsRestClassifier(clf))\n",
        "                ])\n",
        "\n",
        "    model = pipeline.fit(X_train, y_train)\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions, average=\"micro\")\n",
        "\n",
        "    print(clf, accuracy, f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.20      0.33         5\n",
            "           1       0.00      0.00      0.00         0\n",
            "           2       0.00      0.00      0.00         1\n",
            "           3       0.00      0.00      0.00         2\n",
            "           4       0.00      0.00      0.00         0\n",
            "           5       0.00      0.00      0.00         2\n",
            "           6       0.00      0.00      0.00         1\n",
            "           7       0.00      0.00      0.00         1\n",
            "           8       0.00      0.00      0.00         0\n",
            "           9       0.47      0.44      0.45        16\n",
            "          10       0.00      0.00      0.00         4\n",
            "          11       0.00      0.00      0.00         2\n",
            "          12       0.00      0.00      0.00         1\n",
            "          13       0.00      0.00      0.00         0\n",
            "          14       0.00      0.00      0.00         3\n",
            "          15       0.00      0.00      0.00         3\n",
            "          16       0.33      1.00      0.50         1\n",
            "          17       0.00      0.00      0.00         1\n",
            "\n",
            "   micro avg       0.35      0.21      0.26        43\n",
            "   macro avg       0.10      0.09      0.07        43\n",
            "weighted avg       0.30      0.21      0.22        43\n",
            " samples avg       0.22      0.22      0.22        43\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "class_report = classification_report(y_test, predictions)\n",
        "print(class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the classification report it is evident that the imbalanced dataset is causing problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "pickle.dump(model, open(r\"..\\Models\\Determinants\\pipeline_determinants_One-Many_V4-11-05.sav\", 'wb'))\n",
        "\n",
        "# will save the RF model by default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function to get the predictions back from the tf idf classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'numpy.ndarray' object has no attribute 'columns'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlist\u001b[39m(\u001b[43my_determinants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
          ]
        }
      ],
      "source": [
        "list(y_determinants.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['(A)',\n",
              " '(Ad)',\n",
              " '(H)',\n",
              " '(Hd)',\n",
              " 'A',\n",
              " 'Abs',\n",
              " 'Ad',\n",
              " 'Alim',\n",
              " 'Anat',\n",
              " 'Art',\n",
              " 'Bot',\n",
              " 'Elem',\n",
              " 'Frag',\n",
              " 'Ge',\n",
              " 'H',\n",
              " 'Hd',\n",
              " 'Id',\n",
              " 'Nat',\n",
              " 'Obj',\n",
              " 'Pays',\n",
              " 'Radio',\n",
              " 'Sc',\n",
              " 'Sex',\n",
              " 'Sg',\n",
              " 'Vet']"
            ]
          },
          "execution_count": 298,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(y_contents.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the model is tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def evaluate_one_vs_rest_TFIDF(path, text):\n",
        "    \n",
        "    pipeline = pickle.load(open(path, \"rb\"))\n",
        "    \n",
        "    if \"content\" in path:\n",
        "        # print(\"content found\")\n",
        "        possible_outcomes = ['(A)', '(Ad)', '(H)', '(Hd)', 'A', 'Abs', 'Ad', 'Alim', 'Anat', 'Art',\n",
        "       'Bot', 'Elem', 'Frag', 'Ge', 'H', 'Hd', 'Id', 'Nat', 'Obj', 'Pays', 'Radio', 'Sc', 'Sex', 'Sg', 'Vet']\n",
        "        \n",
        "    elif \"determinant\" in path:\n",
        "        # print(\"determinant found\")\n",
        "        possible_outcomes = ['C', 'C\\'', 'C\\'F', 'CF', 'E', 'EF', 'F', 'FC', 'FC\\'', 'FE', 'K', 'kan']\n",
        "\n",
        "    prediction = pipeline.predict([text])\n",
        "    probabilities = pipeline.predict_proba([text]) # sometimes no prediction is given back so we can take the outcome with the highest P instead\n",
        "\n",
        "    # print(\"prediction:\", prediction)\n",
        "    # print(\"probabilities:\", probabilities)\n",
        "    \n",
        "    list_predictions = prediction.tolist()\n",
        "    list_predictions = [x for sublist in list_predictions for x in sublist] # avoid lists with sublists\n",
        "\n",
        "    \n",
        "    if len(list_predictions) != len(possible_outcomes): # sanity check\n",
        "        print(prediction)\n",
        "        print( len(list_predictions)  )\n",
        "        print(possible_outcomes)\n",
        "        print( len(possible_outcomes)  )\n",
        "        print(\"Error encountered in the predictions\")\n",
        "        \n",
        "    results = ([possible_outcomes[i] for i in range(len(list_predictions)) if list_predictions[i] == 1]) \n",
        "\n",
        "    if results == []:\n",
        "        # print(\"No result\")\n",
        "        i = probabilities.argmax(1).item()\n",
        "        # print(ix)\n",
        "        final_results = possible_outcomes[i]\n",
        "    \n",
        "    else:\n",
        "        final_results = str(results).replace(\"\\'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "    \n",
        "    return final_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'A'"
            ]
          },
          "execution_count": 300,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_one_vs_rest_TFIDF(r\"..\\Models\\Contents\\pipeline_contents_One-Many_V3-18-04.sav\", \"Dog\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'FE, kan'"
            ]
          },
          "execution_count": 301,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_one_vs_rest_TFIDF(r\"..\\Models\\Determinants\\pipeline_determinants_One-Many_V3-18-04.sav\", \"Dog\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model development with SentenceTransformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df_contents[\"Answer (English)\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dtype('O')"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = X.astype(\"str\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "embeddigngs_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_transformers = X.apply(lambda x: embeddigngs_model.encode(x, convert_to_numpy=True)) # getting the embeddings for each row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      [0.01710045, 0.032927256, -0.050096795, -0.014...\n",
              "1      [0.04927347, 0.06730551, -0.03920415, -0.03209...\n",
              "2      [0.003027574, 0.03194066, -0.09189637, 0.05275...\n",
              "3      [0.07981707, -0.054176375, 0.07149547, 0.06627...\n",
              "4      [0.017808866, -0.040367622, 0.032963585, -0.01...\n",
              "                             ...                        \n",
              "375    [-0.022829905, 0.06105979, 0.04656104, 0.01641...\n",
              "376    [-0.0058849268, 0.049261548, 0.060328465, -0.0...\n",
              "377    [-0.06306299, 0.031928115, 0.037187733, 0.0149...\n",
              "378    [-0.023190409, -0.007055615, 0.027367428, 0.03...\n",
              "379    [-0.01656596, -0.005031914, 0.016621804, 0.037...\n",
              "Name: Answer (English), Length: 380, dtype: object"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_transformers = X_transformers.apply(lambda x: np.mean(x)) # getting the centroid of each embedding array, we cna't just feed the classifier vectors\n",
        "X_transformers = np.array(X_transformers).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-1.7147581e-03],\n",
              "       [-1.3931376e-03],\n",
              "       [-2.7161246e-05],\n",
              "       [ 3.6774381e-04],\n",
              "       [-2.1004721e-03]], dtype=float32)"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_transformers[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sentence Transfomers model for Determinants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNeighborsClassifier() 0.05263157894736842 0.07017543859649122\n",
            "LogisticRegression(random_state=42, solver='sag') 0.0 0.0\n",
            "SVC() 0.0 0.0\n",
            "RandomForestClassifier(random_state=42) 0.18421052631578946 0.17777777777777776\n",
            "SGDClassifier(random_state=42) 0.0 0.0\n",
            "GradientBoostingClassifier(random_state=42) 0.10526315789473684 0.10256410256410256\n"
          ]
        }
      ],
      "source": [
        "y = y_determinants\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_transformers, y, test_size = 0.10, random_state = 42)\n",
        "\n",
        "for clf in models:\n",
        "        \n",
        "        pipeline = Pipeline([\n",
        "                        ('clf', OneVsRestClassifier(clf)),\n",
        "                ])\n",
        "\n",
        "        model = pipeline.fit(X_train, y_train)\n",
        "\n",
        "        predictions = model.predict(X_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, predictions)\n",
        "        f1 = f1_score(y_test, predictions, average=\"micro\")\n",
        "\n",
        "        print(clf, accuracy, f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.20      0.22         5\n",
            "           1       0.00      0.00      0.00         0\n",
            "           2       0.00      0.00      0.00         1\n",
            "           3       0.00      0.00      0.00         2\n",
            "           4       0.00      0.00      0.00         0\n",
            "           5       0.00      0.00      0.00         2\n",
            "           6       0.00      0.00      0.00         1\n",
            "           7       0.00      0.00      0.00         1\n",
            "           8       0.00      0.00      0.00         0\n",
            "           9       0.56      0.31      0.40        16\n",
            "          10       0.25      0.50      0.33         4\n",
            "          11       0.00      0.00      0.00         2\n",
            "          12       0.00      0.00      0.00         1\n",
            "          13       0.00      0.00      0.00         0\n",
            "          14       0.00      0.00      0.00         3\n",
            "          15       0.00      0.00      0.00         3\n",
            "          16       0.00      0.00      0.00         1\n",
            "          17       0.00      0.00      0.00         1\n",
            "\n",
            "   micro avg       0.17      0.19      0.18        43\n",
            "   macro avg       0.06      0.06      0.05        43\n",
            "weighted avg       0.26      0.19      0.21        43\n",
            " samples avg       0.21      0.20      0.20        43\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "class_report = classification_report(y_test, predictions)\n",
        "print(class_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "pickle.dump(model, open(r\"..\\Models\\Determinants\\sentence_transformer_determinants_V6-11-05.sav\", 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sentence Transfomers model for Contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNeighborsClassifier() 0.05263157894736842 0.10344827586206898\n",
            "LogisticRegression(random_state=42, solver='sag') 0.0 0.0\n",
            "SVC() 0.0 0.0\n",
            "RandomForestClassifier(random_state=42) 0.07894736842105263 0.08333333333333333\n",
            "SGDClassifier(random_state=42) 0.0 0.0\n",
            "GradientBoostingClassifier(random_state=42) 0.07894736842105263 0.09090909090909091\n"
          ]
        }
      ],
      "source": [
        "y = y_contents\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_transformers, y, test_size = 0.10, random_state = 42)\n",
        "\n",
        "for clf in models:\n",
        "        \n",
        "        pipeline = Pipeline([\n",
        "                        ('clf', OneVsRestClassifier(clf)),\n",
        "                ])\n",
        "\n",
        "        model = pipeline.fit(X_train, y_train)\n",
        "\n",
        "        predictions = model.predict(X_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, predictions)\n",
        "        f1 = f1_score(y_test, predictions, average=\"micro\")\n",
        "\n",
        "        print(clf, accuracy, f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         4\n",
            "           1       0.00      0.00      0.00         0\n",
            "           2       0.00      0.00      0.00         4\n",
            "           3       0.00      0.00      0.00         1\n",
            "           4       0.38      0.30      0.33        10\n",
            "           5       0.00      0.00      0.00         1\n",
            "           6       0.00      0.00      0.00         4\n",
            "           7       0.00      0.00      0.00         1\n",
            "           8       0.00      0.00      0.00         0\n",
            "           9       0.00      0.00      0.00         0\n",
            "          10       0.00      0.00      0.00         0\n",
            "          11       0.00      0.00      0.00         2\n",
            "          12       0.00      0.00      0.00         0\n",
            "          13       0.00      0.00      0.00         3\n",
            "          14       0.00      0.00      0.00         0\n",
            "          15       0.00      0.00      0.00         2\n",
            "          16       0.00      0.00      0.00         4\n",
            "          17       0.00      0.00      0.00         1\n",
            "          18       0.00      0.00      0.00         0\n",
            "          19       0.00      0.00      0.00         4\n",
            "          20       0.00      0.00      0.00         1\n",
            "          21       0.00      0.00      0.00         0\n",
            "          22       0.00      0.00      0.00         0\n",
            "          23       0.00      0.00      0.00         0\n",
            "          24       0.00      0.00      0.00         0\n",
            "          25       0.50      0.50      0.50         2\n",
            "          26       0.00      0.00      0.00         1\n",
            "\n",
            "   micro avg       0.08      0.09      0.08        45\n",
            "   macro avg       0.03      0.03      0.03        45\n",
            "weighted avg       0.11      0.09      0.10        45\n",
            " samples avg       0.11      0.09      0.09        45\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "class_report = classification_report(y_test, predictions)\n",
        "print(class_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "pickle.dump(model, open(r\"..\\Models\\Contents\\sentence_transformer_contents_V6-11-05.sav\", 'wb'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function to get the label from the Sentence Transformer classifier (in progress)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text_for_transformer(text):\n",
        "    \n",
        "    embeddings_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    \n",
        "    x_array = embeddings_model.encode(text, convert_to_numpy=True)\n",
        "    \n",
        "    x_centroid = np.mean(x_array)\n",
        "    X_transformers = x_centroid.reshape(-1,1)\n",
        "\n",
        "    \n",
        "    return X_transformers[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.0003033], dtype=float32)"
            ]
          },
          "execution_count": 367,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocess_text_for_transformer(\"dog with two tails\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def evaluate_one_vs_rest_transformer(path, text):\n",
        "    \n",
        "    pipeline = pickle.load(open(path, \"rb\"))\n",
        "    \n",
        "    if \"content\" in path:\n",
        "        # print(\"content found\")\n",
        "        possible_outcomes = ['(A)', '(Ad)', '(H)', '(Hd)', 'A', 'Abs', 'Ad', 'Alim', 'Anat', 'Art',\n",
        "       'Bot', 'Elem', 'Frag', 'Ge', 'H', 'Hd', 'Id', 'Nat', 'Obj', 'Pays', 'Radio', 'Sc', 'Sex', 'Sg', 'Vet']\n",
        "        \n",
        "    elif \"determinant\" in path:\n",
        "        # print(\"determinant found\")\n",
        "        possible_outcomes = ['C', 'C\\'', 'C\\'F', 'CF', 'E', 'EF', 'F', 'FC', 'FC\\'', 'FE', 'K', 'kan']\n",
        "\n",
        "    text_transformed = preprocess_text_for_transformer(text)\n",
        "    \n",
        "    prediction = pipeline.predict([text_transformed])\n",
        "    probabilities = pipeline.predict_proba([text_transformed]) # sometimes no prediction is given back so we can take the outcome with the highest P instead\n",
        "\n",
        "    # print(\"prediction:\", prediction)\n",
        "    # print(\"probabilities:\", probabilities)\n",
        "    \n",
        "    list_predictions = prediction.tolist()\n",
        "    list_predictions = [x for sublist in list_predictions for x in sublist] # avoid lists with sublists\n",
        "\n",
        "    \n",
        "    if len(list_predictions) != len(possible_outcomes): # sanity check\n",
        "        print(prediction)\n",
        "        print( len(list_predictions)  )\n",
        "        print(possible_outcomes)\n",
        "        print( len(possible_outcomes)  )\n",
        "        print(\"Error encountered in the predictions\")\n",
        "        \n",
        "    results = ([possible_outcomes[i] for i in range(len(list_predictions)) if list_predictions[i] == 1]) \n",
        "\n",
        "    if results == []:\n",
        "        # print(\"No result\")\n",
        "        i = probabilities.argmax(1).item()\n",
        "        # print(ix)\n",
        "        final_results = possible_outcomes[i]\n",
        "    \n",
        "    else:\n",
        "        final_results = str(results).replace(\"\\'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "    \n",
        "    return final_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.00023756], dtype=float32)"
            ]
          },
          "execution_count": 369,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocess_text_for_transformer(\"dog with tail\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'A, Anat'"
            ]
          },
          "execution_count": 370,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_one_vs_rest_transformer(r\"..\\Models\\Contents\\sentence_transformer_contents_V23-18-04.sav\", \"Dog with tail\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'F, FE'"
            ]
          },
          "execution_count": 371,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_one_vs_rest_transformer(r\"..\\Models\\Determinants\\sentence_transformer_determinants_V23-18-04.sav\", \"Dog with tail\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
