{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-GV8nLI2AXD",
        "outputId": "ab0d0d62-193c-4b69-fe71-19944ae2dba3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BRCz44s2_5s",
        "outputId": "3d71850a-4971-4a08-a12a-7df12a143b2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cpNn76Ej2b7Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig, BertModel\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig, DistilBertModel\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForMaskedLM\n",
        "from transformers import LongformerConfig, LongformerTokenizer, LongformerModel\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics.cluster import contingency_matrix\n",
        "\n",
        "import shutil\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NAEtmzK-AIRj"
      },
      "outputs": [],
      "source": [
        "def pre_process_data(df, test_proportion, train_size):\n",
        "\n",
        "    df.columns.values[0] = \"FQText\"\n",
        "    size_df = df.shape[0]\n",
        "    df = shuffle(df, random_state = 42)\n",
        "    X = df[[\"FQText\"]]\n",
        "    y = df.drop([\"FQText\"], axis=1).astype(np.float32)\n",
        "    print(y.info())\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_proportion, shuffle=True, random_state=42)\n",
        "\n",
        "    df_train = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
        "    df_test = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n",
        "    df_labels = list(y.columns)\n",
        "\n",
        "    print(df_labels)\n",
        "\n",
        "    train_df = df_train.sample(frac=train_size, random_state=42).reset_index(drop=True)\n",
        "    val_df = df_train.drop(train_df.index).reset_index(drop=True)\n",
        "\n",
        "    print(\"Total amount of data: {}\".format(size_df))\n",
        "    print(\"Number of rows used to TRAIN: {}\".format(train_df.shape[0]))\n",
        "    print(\"Number of rows used to VALIDATE: {}\".format(val_df.shape[0]))\n",
        "    print(\"Number of rows used to TEST: {}\".format(df_test.shape[0]))\n",
        "\n",
        "    return train_df, val_df, df_test, df_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "020inJJS15jg"
      },
      "outputs": [],
      "source": [
        "def set_hyperparams(hp_dictionary):\n",
        "\n",
        "    MAX_LEN = hp_dictionary[\"MAX_LEN\"]\n",
        "    TRAIN_BATCH_SIZE = hp_dictionary[\"TRAIN_BATCH_SIZE\"]\n",
        "    VALID_BATCH_SIZE = hp_dictionary[\"VALID_BATCH_SIZE\"]\n",
        "    EPOCHS = hp_dictionary[\"EPOCHS\"]\n",
        "    LEARNING_RATE = hp_dictionary[\"LEARNING_RATE\"]\n",
        "\n",
        "    return MAX_LEN, TRAIN_BATCH_SIZE, VALID_BATCH_SIZE, EPOCHS, LEARNING_RATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MFhEjXxU15jg"
      },
      "outputs": [],
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into\n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss\n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "By6QjBoA15jh"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['FQText']\n",
        "        # self.labels = list(df.columns)[2:] # list of the target values\n",
        "        self.targets = self.df[df_labels].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.title[index])\n",
        "        title = \" \".join(title.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True, # do we actually need special tokens ??\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'  # pytorch tensors\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JfND4miy15jh"
      },
      "outputs": [],
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        # self.bert_model = LongformerModel.from_pretrained('allenai/longformer-base-4096', return_dict=True, problem_type=\"multi_label_classification\")\n",
        "        self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\", return_dict=True, problem_type=\"multi_label_classification\")\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(0.3) # why 0.3?\n",
        "        self.linear = torch.nn.Linear(768, len(df_labels)) # have to changet he n of possible labels here\n",
        "\n",
        "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
        "        output = self.bert_model(\n",
        "            input_ids,\n",
        "            attention_mask=attn_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        # the issue is that bert gets size 6 here?\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "        # print(output_dropout)\n",
        "        output = self.linear(output_dropout)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rD0OObyw15jh"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    # print(outputs, targets)\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAFnT38GBjUr"
      },
      "source": [
        "## Train Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "1inxJZtT15jh"
      },
      "outputs": [],
      "source": [
        "def train_model(n_epochs, training_loader, validation_loader, model,\n",
        "                optimizer, checkpoint_path, best_model_path, df_labels):\n",
        "  # initialize tracker for minimum validation loss\n",
        "  valid_loss_min = np.Inf\n",
        "\n",
        "  val_targets_study= []\n",
        "  val_outputs_study= [] # I am going to extend all the validation probabilities to determine what the prob threshold should be\n",
        "\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
        "    for batch_idx, data in enumerate(training_loader):\n",
        "        # print('yyy epoch', batch_idx)\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        \"\"\"targets are the y array of the original data.\n",
        "        \"\"\"\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        #print('after loss data in training', loss.item(), train_loss)\n",
        "\n",
        "\n",
        "\n",
        "    print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "\n",
        "    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################\n",
        "    # validate the model #\n",
        "    ######################\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    val_targets = []\n",
        "    val_outputs = []\n",
        "\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(validation_loader, 0):\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "        val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "        val_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
        "\n",
        "        probs = sigmoid(torch.Tensor(outputs))\n",
        "        # print(probs)\n",
        "\n",
        "        val_targets_study.append(targets.cpu().detach().numpy().tolist())\n",
        "        val_outputs_study.append(probs.cpu().detach().numpy().tolist())\n",
        "\n",
        "        # outputs = model(ids.squeeze(), mask.squeeze(), token_type_ids.squeeze())\n",
        "\n",
        "\n",
        "\n",
        "      #       metrics_targets.extend(torch.argmax(softm(targets), dim=1).cpu().detach().numpy().tolist())\n",
        "      #       metrics_outputs.extend(torch.argmax(softm(outputs), dim=1).cpu().detach().numpy().tolist())\n",
        "\n",
        "      # metrics_outputs = np.array(metrics_outputs, dtype=int)\n",
        "      # val_f1 = f1_score(metrics_outputs, metrics_targets, average=\"weighted\")\n",
        "      # val_acc = accuracy_score(metrics_outputs, metrics_targets)\n",
        "      # class_report = classification_report(metrics_outputs, metrics_targets, target_names = df_labels)\n",
        "\n",
        "      # print((f\"Accuracy: {val_f1}\"))\n",
        "      # print((f\"F1 Score (Weighted): {val_f1}\"))\n",
        "      # print((f\"Classification report: \\n{class_report}\"))\n",
        "\n",
        "\n",
        "\n",
        "      print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "\n",
        "      # calculate average losses\n",
        "      #print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics\n",
        "      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch,\n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "\n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "\n",
        "        # save checkpoint\n",
        "      save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "\n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      # if valid_loss <= valid_loss_min:\n",
        "      #   print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "      #   # save checkpoint as best model\n",
        "      #   save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "      #   valid_loss_min = valid_loss\n",
        "\n",
        "\n",
        "\n",
        "    print('############# Epoch {}  Metrics   #############\\n\\n'.format(epoch))\n",
        "    metrics = multi_labels_metrics(val_outputs, val_targets)\n",
        "    # print((f\"EVAL METRICS: {metrics}\\n\"))\n",
        "\n",
        "    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "  return model, val_targets_study, val_outputs_study"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "FuJfhnbd8gU4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "9OJEqgab09XX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "3C3Cl8UnqkrC"
      },
      "outputs": [],
      "source": [
        "# not used at the moment\n",
        "\n",
        "# def compute_metrics(epoch, validation_loader):\n",
        "#   model.eval()\n",
        "#   metrics_targets = []\n",
        "#   metrics_outputs = []\n",
        "#   softm = torch.nn.Softmax(dim=1)\n",
        "#   with torch.no_grad():\n",
        "#         for _, data in enumerate(validation_loader, 0):\n",
        "#               ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "#               mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "#               token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "#               targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "#               outputs = model(ids.squeeze(), mask.squeeze(), token_type_ids.squeeze())\n",
        "\n",
        "#               metrics_targets.extend(torch.argmax(softm(targets), dim=1).cpu().detach().numpy().tolist())\n",
        "#               metrics_outputs.extend(torch.argmax(softm(outputs), dim=1).cpu().detach().numpy().tolist())\n",
        "\n",
        "#   return metrics_targets, metrics_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "-LLrqlZuvVRR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-rvtFb1ByjV"
      },
      "source": [
        "TRAIN AND VALIDATION LOOP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utSjW4nUBudm"
      },
      "source": [
        "# Determinants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "YO1ZAHm2pH0Q"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"nancy_determinants_grouped.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1QmCorGB56k"
      },
      "source": [
        "GROUPED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "3k2rFrs5B3eL"
      },
      "outputs": [],
      "source": [
        "df.rename(columns = {\"Answer (English)\": \"FQText\"}, inplace=True)\n",
        "\n",
        "df.drop(['Déterminant', 'C', 'C\\'', 'C\\'F',\n",
        "       'CF', 'CF\\'', 'CLOB', 'CLOBF', 'E', 'EF', 'F', 'FC', 'FC\\'', 'FCLOB',\n",
        "       'FE', 'K', 'KAN', 'KOB', 'KP', 'Réponse (French)'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "NwsqVD3wpIQ1"
      },
      "outputs": [],
      "source": [
        "# df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d7u4ywFCRJP"
      },
      "source": [
        "INDIVIDUAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "9BV42umwB_A6"
      },
      "outputs": [],
      "source": [
        "# df.rename(columns = {\"Answer (English)\": \"FQText\"}, inplace=True)\n",
        "\n",
        "# df.drop(['Déterminant', 'color_sum', 'threat_sum', 'fading_sum',\n",
        "#        'form_sum', 'kinesthetics_sum', 'Réponse (French)'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "8jWuAeE8CGVT"
      },
      "outputs": [],
      "source": [
        "# df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsrPLGg4CIyd"
      },
      "source": [
        "# Contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "cbDvsEuK43vS"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_csv(\"nancy_contents_grouped.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebg_fC5uBy5q"
      },
      "source": [
        "GROUPED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "H85aZstXCVRA"
      },
      "outputs": [],
      "source": [
        "# df.rename(columns = {\"Answer (English)\": \"FQText\"}, inplace=True)\n",
        "\n",
        "# df.drop(['Contenu', '(A)', '(AD)', '(H)', '(HD)',\n",
        "#        'A', 'ABS', 'AD', 'ALIM', 'ANAT', 'ARCH', 'ART', 'BOT', 'ELEM', 'FRAG',\n",
        "#        'GÉO', 'H', 'HD', 'MQ', 'NAT', 'OBJ', 'PAYS', 'RADIO', 'SC', 'SCÈNE',\n",
        "#        'SEX', 'SG', 'VÊT', 'Réponse (French)'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "nJkQypP1Aghv"
      },
      "outputs": [],
      "source": [
        "# df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIghMIOuCSZ0"
      },
      "source": [
        "INDIVIDUAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "-vKjiDTBCZG-"
      },
      "outputs": [],
      "source": [
        "# df.rename(columns = {\"Answer (English)\": \"FQText\"}, inplace=True)\n",
        "\n",
        "# df.drop(['Contenu', 'animal_sum', 'human_sum', 'abs_sum',\n",
        "#        'food_sum', 'art_arch_sum', 'nature_sum', 'fragment_sum', 'geo_sum',\n",
        "#        'object_sum', 'science_sum', 'graphic_sum', 'Réponse (French)'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "rqSzXgVtClTR"
      },
      "outputs": [],
      "source": [
        "# df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDCIfjfOCtCP"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "HOx4W-iAplm2"
      },
      "outputs": [],
      "source": [
        "hp_dictionary = {\"MAX_LEN\":512 ,\n",
        "    \"TRAIN_BATCH_SIZE\": 16,\n",
        "    \"VALID_BATCH_SIZE\": 16,\n",
        "    \"EPOCHS\": 4,\n",
        "    \"LEARNING_RATE\": 1e-05\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3Jrv-cgjdnY"
      },
      "source": [
        "CHANGE PROBLEM TYPE AQUI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thCkEarT_sEz",
        "outputId": "ea900334-5ad3-41d4-8527-30ed6bc1a8db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096', problem_type=\"multi_label_classification\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", problem_type=\"multi_label_classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-BxMEEb15jh",
        "outputId": "8ea0c277-0b88-4c82-d193-a456325da65a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 380 entries, 266 to 102\n",
            "Data columns (total 5 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   color_sum         380 non-null    float32\n",
            " 1   threat_sum        380 non-null    float32\n",
            " 2   fading_sum        380 non-null    float32\n",
            " 3   form_sum          380 non-null    float32\n",
            " 4   kinesthetics_sum  380 non-null    float32\n",
            "dtypes: float32(5)\n",
            "memory usage: 10.4 KB\n",
            "None\n",
            "['color_sum', 'threat_sum', 'fading_sum', 'form_sum', 'kinesthetics_sum']\n",
            "Total amount of data: 380\n",
            "Number of rows used to TRAIN: 291\n",
            "Number of rows used to VALIDATE: 51\n",
            "Number of rows used to TEST: 38\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "MAX_LEN, TRAIN_BATCH_SIZE, VALID_BATCH_SIZE, EPOCHS, LEARNING_RATE = set_hyperparams(hp_dictionary)\n",
        "\n",
        "test_proportion = 0.1\n",
        "test_size = 0.85\n",
        "\n",
        "train_df, val_df, df_test, df_labels  = pre_process_data(df, test_proportion, test_size)\n",
        "\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "proba_threshold = 0.5\n",
        "\n",
        "\n",
        "ckpt_path = \"curr_ckpt\"\n",
        "best_model_path = \"best_model.pt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J9n1jlpXfQK"
      },
      "source": [
        "Counting the labels present in each train/test/val batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "dlmtE6LEWSIi"
      },
      "outputs": [],
      "source": [
        "# def count_instances(y):\n",
        "#   total_labels = 0\n",
        "#   for col in y:\n",
        "#       if col == \"FQText\":\n",
        "#           continue\n",
        "#       else:\n",
        "#           sum_examples = y[col].sum()\n",
        "#           print(col, sum_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "uDeEu92CWXhp"
      },
      "outputs": [],
      "source": [
        "# y_val = val_df.drop(\"FQText\", axis=1)\n",
        "# y_train = train_df.drop(\"FQText\", axis=1)\n",
        "# y_test = df_test.drop(\"FQText\", axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "OzlseIbCW20h"
      },
      "outputs": [],
      "source": [
        "# print(\"train class distribution\".upper())\n",
        "# print(\"\")\n",
        "# print(count_instances(y_train))\n",
        "# print(\"total size of the test data\", len(y_train))\n",
        "# print(\"\\nvalidation class distribution\".upper())\n",
        "# print(\"\")\n",
        "# print(count_instances(y_val))\n",
        "# print(\"total size of the test data\", len(y_val))\n",
        "# print(\"\\ntest class distribution\".upper())\n",
        "# print(\"\")\n",
        "# print(count_instances(y_test))\n",
        "# print(\"total size of the test data\", len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNKen3HtcgrN"
      },
      "source": [
        "https://discuss.huggingface.co/t/dataset-label-format-for-multi-label-text-classification/14998\n",
        "\n",
        "\n",
        "https://colab.research.google.com/drive/1aue7x525rKy6yYLqqt-5Ll96qjQvpqS7#scrollTo=6d869uZsT9MH\n",
        "\n",
        "https://discuss.huggingface.co/t/multi-class-using-dataset/8970/3\n",
        "\n",
        "https://www.youtube.com/watch?v=ZYc9za75Chk\n",
        "\n",
        "https://arxiv.org/abs/2004.05150\n",
        "\n",
        "https://huggingface.co/docs/transformers/model_doc/longformer\n",
        "\n",
        "\n",
        "https://www.evidentlyai.com/classification-metrics/classification-threshold  - precision recall curve to set prob thresholds, class separation quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "NA24sYKp8tvE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, f1_score, hamming_loss\n",
        "from transformers import EvalPrediction\n",
        "import torch\n",
        "\n",
        "\n",
        "def multi_labels_metrics(predictions, targets, threshold=0.355, df_labels=df_labels): # mess with threshold\n",
        "  sigmoid = torch.nn.Sigmoid()\n",
        "  probs = sigmoid(torch.Tensor(predictions)) #mapply activation function in the raw values\n",
        "\n",
        "  y_pred = np.zeros(probs.shape)\n",
        "  y_pred[np.where(probs>=threshold)] = 1\n",
        "  y_true = targets\n",
        "\n",
        "  f1 = f1_score(y_true, y_pred, average = 'micro')\n",
        "  roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
        "  hamming = hamming_loss(y_true, y_pred)\n",
        "  class_report = classification_report(y_true, y_pred, target_names=df_labels)\n",
        "\n",
        "  metrics = {\n",
        "      \"roc_auc\": roc_auc,       # special multilabel metrics\n",
        "      \"hamming_loss\": hamming,  # special multilabel metrics\n",
        "      \"f1\": f1\n",
        "  }\n",
        "  print(metrics)\n",
        "  print(class_report)\n",
        "  return metrics\n",
        "\n",
        "def compute_test_metrics(test_loader, val_outputs_study, val_targets_study):\n",
        "\n",
        "  model.eval()\n",
        "  metrics_targets = []\n",
        "  metrics_outputs = []\n",
        "  sigmoid = torch.nn.Sigmoid()\n",
        "  with torch.no_grad():\n",
        "        for _, data in enumerate(test_loader, 0):\n",
        "              ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "              mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "              token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "              targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "              outputs = model(ids.squeeze(), mask.squeeze(), token_type_ids.squeeze())\n",
        "\n",
        "              metrics_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "              metrics_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
        "\n",
        "\n",
        "              probs = sigmoid(torch.Tensor(outputs))\n",
        "              # print(probs)\n",
        "\n",
        "              val_targets_study.append(targets.cpu().detach().numpy().tolist())\n",
        "              val_outputs_study.append(probs.cpu().detach().numpy().tolist())\n",
        "\n",
        "  metrics = multi_labels_metrics(metrics_outputs, metrics_targets)\n",
        "  print((f\"EVAL METRICS: {metrics}\"))\n",
        "\n",
        "  return metrics, val_outputs_study, val_targets_study\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFcH7SslBUkn"
      },
      "source": [
        "## TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFeKHNTX15ji",
        "outputId": "f49a57b2-e629-447e-91db-1d73f83e9759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "############# Epoch 1: Training Start   #############\n",
            "############# Epoch 1: Training End     #############\n",
            "############# Epoch 1: Validation Start   #############\n",
            "tensor([[0.4227, 0.1626, 0.2670, 0.4591, 0.3432],\n",
            "        [0.4448, 0.1679, 0.2875, 0.4313, 0.3179],\n",
            "        [0.4192, 0.1600, 0.2555, 0.4413, 0.3339],\n",
            "        [0.4191, 0.1763, 0.2852, 0.4401, 0.3291],\n",
            "        [0.4481, 0.1638, 0.2705, 0.5664, 0.3447],\n",
            "        [0.4808, 0.1768, 0.2706, 0.5990, 0.3380],\n",
            "        [0.4931, 0.1523, 0.2511, 0.4856, 0.2914],\n",
            "        [0.4746, 0.1660, 0.2712, 0.5696, 0.3069],\n",
            "        [0.4105, 0.1479, 0.2760, 0.4388, 0.3124],\n",
            "        [0.4311, 0.1588, 0.2629, 0.5440, 0.3454],\n",
            "        [0.4700, 0.1637, 0.2601, 0.5389, 0.3226],\n",
            "        [0.4527, 0.1558, 0.2671, 0.4235, 0.3079],\n",
            "        [0.4252, 0.1765, 0.2734, 0.5674, 0.3304],\n",
            "        [0.4554, 0.1646, 0.2714, 0.5965, 0.3475],\n",
            "        [0.4821, 0.1905, 0.2887, 0.4400, 0.3090],\n",
            "        [0.4104, 0.1549, 0.2823, 0.5493, 0.3633]], device='cuda:0')\n",
            "tensor([[0.4470, 0.1594, 0.2426, 0.5049, 0.3162],\n",
            "        [0.4239, 0.1623, 0.2673, 0.4370, 0.3243],\n",
            "        [0.5117, 0.1542, 0.2448, 0.5242, 0.3003],\n",
            "        [0.4047, 0.1892, 0.2750, 0.4685, 0.3564],\n",
            "        [0.4402, 0.1637, 0.2524, 0.5089, 0.3244],\n",
            "        [0.4886, 0.1540, 0.2614, 0.4323, 0.3009],\n",
            "        [0.4394, 0.1528, 0.2527, 0.4488, 0.2955],\n",
            "        [0.5289, 0.1793, 0.2957, 0.4065, 0.2893],\n",
            "        [0.4517, 0.1689, 0.2604, 0.4865, 0.3249],\n",
            "        [0.4303, 0.1714, 0.2715, 0.5150, 0.3377],\n",
            "        [0.4899, 0.1920, 0.3099, 0.3712, 0.3085],\n",
            "        [0.4456, 0.1695, 0.2707, 0.4797, 0.3203],\n",
            "        [0.4766, 0.1599, 0.2479, 0.5101, 0.3347],\n",
            "        [0.4750, 0.1923, 0.2822, 0.4303, 0.3233],\n",
            "        [0.4369, 0.1732, 0.2879, 0.5782, 0.3497],\n",
            "        [0.4720, 0.1603, 0.2566, 0.4080, 0.3017]], device='cuda:0')\n",
            "tensor([[0.4717, 0.1649, 0.2806, 0.5470, 0.3066],\n",
            "        [0.4222, 0.1664, 0.2728, 0.4201, 0.3344],\n",
            "        [0.4135, 0.1574, 0.2505, 0.4813, 0.3560],\n",
            "        [0.4458, 0.1753, 0.2371, 0.4716, 0.3367],\n",
            "        [0.4308, 0.1594, 0.2562, 0.5052, 0.3399],\n",
            "        [0.4019, 0.1585, 0.2441, 0.5049, 0.3444],\n",
            "        [0.3881, 0.1833, 0.2833, 0.4416, 0.3664],\n",
            "        [0.4027, 0.1679, 0.2433, 0.4569, 0.3324],\n",
            "        [0.3859, 0.1845, 0.2777, 0.5249, 0.3604],\n",
            "        [0.4089, 0.1571, 0.2550, 0.4821, 0.3516],\n",
            "        [0.3600, 0.1865, 0.2865, 0.5124, 0.3710],\n",
            "        [0.4292, 0.1804, 0.3019, 0.5600, 0.3489],\n",
            "        [0.4317, 0.1659, 0.2782, 0.4851, 0.3208],\n",
            "        [0.4419, 0.1985, 0.3009, 0.4120, 0.3474],\n",
            "        [0.4198, 0.1626, 0.2492, 0.4905, 0.3463],\n",
            "        [0.3828, 0.1813, 0.2723, 0.5129, 0.3566]], device='cuda:0')\n",
            "tensor([[0.4922, 0.2405, 0.3485, 0.6733, 0.3836],\n",
            "        [0.4533, 0.1467, 0.2461, 0.4546, 0.3203],\n",
            "        [0.4630, 0.1588, 0.2502, 0.4675, 0.3133]], device='cuda:0')\n",
            "############# Epoch 1: Validation End     #############\n",
            "Epoch: 1 \tAvgerage Training Loss: 0.029000 \tAverage Validation Loss: 0.129548\n",
            "############# Epoch 1  Metrics   #############\n",
            "\n",
            "\n",
            "{'roc_auc': 0.6370320855614973, 'hamming_loss': 0.3607843137254902, 'f1': 0.48314606741573035}\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       color_sum       0.43      1.00      0.60        22\n",
            "      threat_sum       0.00      0.00      0.00         3\n",
            "      fading_sum       0.00      0.00      0.00         6\n",
            "        form_sum       0.35      1.00      0.52        18\n",
            "kinesthetics_sum       0.38      0.16      0.22        19\n",
            "\n",
            "       micro avg       0.39      0.63      0.48        68\n",
            "       macro avg       0.23      0.43      0.27        68\n",
            "    weighted avg       0.34      0.63      0.40        68\n",
            "     samples avg       0.39      0.71      0.49        68\n",
            "\n",
            "############# Epoch 1  Done   #############\n",
            "\n",
            "############# Epoch 2: Training Start   #############\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "############# Epoch 2: Training End     #############\n",
            "############# Epoch 2: Validation Start   #############\n",
            "tensor([[0.3428, 0.1534, 0.2238, 0.3461, 0.3710],\n",
            "        [0.3800, 0.1538, 0.2455, 0.3299, 0.3410],\n",
            "        [0.3487, 0.1356, 0.2007, 0.3458, 0.3277],\n",
            "        [0.3671, 0.1613, 0.2455, 0.3411, 0.3270],\n",
            "        [0.3372, 0.1290, 0.1883, 0.4307, 0.3090],\n",
            "        [0.3984, 0.1219, 0.1802, 0.5026, 0.2751],\n",
            "        [0.4140, 0.1314, 0.1870, 0.3204, 0.2759],\n",
            "        [0.4149, 0.1231, 0.1890, 0.5118, 0.2481],\n",
            "        [0.3497, 0.1464, 0.2358, 0.3121, 0.3238],\n",
            "        [0.3457, 0.1318, 0.1858, 0.3846, 0.3080],\n",
            "        [0.4023, 0.1192, 0.1710, 0.4342, 0.2605],\n",
            "        [0.4181, 0.1423, 0.2304, 0.3078, 0.3055],\n",
            "        [0.3608, 0.1293, 0.1851, 0.4855, 0.2636],\n",
            "        [0.3408, 0.1255, 0.1781, 0.4554, 0.3079],\n",
            "        [0.4302, 0.1514, 0.2249, 0.3547, 0.2857],\n",
            "        [0.3241, 0.1253, 0.1954, 0.4249, 0.3371]], device='cuda:0')\n",
            "tensor([[0.3754, 0.1398, 0.1799, 0.3611, 0.2975],\n",
            "        [0.3642, 0.1490, 0.2305, 0.3283, 0.3261],\n",
            "        [0.4493, 0.1129, 0.1625, 0.3850, 0.2472],\n",
            "        [0.3289, 0.1694, 0.2211, 0.3756, 0.3634],\n",
            "        [0.3529, 0.1353, 0.1801, 0.4028, 0.3026],\n",
            "        [0.4450, 0.1470, 0.2276, 0.2958, 0.3063],\n",
            "        [0.3856, 0.1292, 0.1905, 0.3176, 0.2658],\n",
            "        [0.5243, 0.1733, 0.2765, 0.2996, 0.2940],\n",
            "        [0.3809, 0.1279, 0.1889, 0.4098, 0.2873],\n",
            "        [0.3785, 0.1381, 0.2079, 0.4092, 0.2970],\n",
            "        [0.4934, 0.1818, 0.2805, 0.2879, 0.3127],\n",
            "        [0.3569, 0.1426, 0.2116, 0.3874, 0.3155],\n",
            "        [0.4428, 0.1191, 0.1735, 0.4873, 0.2806],\n",
            "        [0.4260, 0.1607, 0.2301, 0.3532, 0.3124],\n",
            "        [0.3660, 0.1260, 0.1989, 0.5162, 0.2886],\n",
            "        [0.4525, 0.1520, 0.2196, 0.2871, 0.3110]], device='cuda:0')\n",
            "tensor([[0.3885, 0.1211, 0.1849, 0.4159, 0.2592],\n",
            "        [0.3717, 0.1547, 0.2292, 0.3161, 0.3423],\n",
            "        [0.3481, 0.1416, 0.2031, 0.3453, 0.3491],\n",
            "        [0.3725, 0.1565, 0.1860, 0.3574, 0.3286],\n",
            "        [0.3218, 0.1506, 0.2074, 0.3731, 0.3726],\n",
            "        [0.3308, 0.1318, 0.1777, 0.3830, 0.3132],\n",
            "        [0.3407, 0.1643, 0.2327, 0.3523, 0.3622],\n",
            "        [0.3389, 0.1439, 0.1880, 0.3592, 0.3174],\n",
            "        [0.3098, 0.1497, 0.2005, 0.4240, 0.3138],\n",
            "        [0.3389, 0.1475, 0.2149, 0.3544, 0.3550],\n",
            "        [0.2941, 0.1632, 0.2235, 0.4095, 0.3528],\n",
            "        [0.3239, 0.1366, 0.2004, 0.4585, 0.3108],\n",
            "        [0.3629, 0.1247, 0.2019, 0.4097, 0.2768],\n",
            "        [0.4293, 0.1859, 0.2704, 0.3294, 0.3487],\n",
            "        [0.3345, 0.1386, 0.1892, 0.4144, 0.3355],\n",
            "        [0.3148, 0.1546, 0.2057, 0.4117, 0.3176]], device='cuda:0')\n",
            "tensor([[0.4103, 0.1726, 0.2480, 0.6253, 0.3058],\n",
            "        [0.3869, 0.1381, 0.2022, 0.3172, 0.3147],\n",
            "        [0.4141, 0.1400, 0.2105, 0.3248, 0.3082]], device='cuda:0')\n",
            "############# Epoch 2: Validation End     #############\n",
            "Epoch: 2 \tAvgerage Training Loss: 0.026713 \tAverage Validation Loss: 0.121314\n",
            "############# Epoch 2  Metrics   #############\n",
            "\n",
            "\n",
            "{'roc_auc': 0.6898395721925134, 'hamming_loss': 0.23529411764705882, 'f1': 0.5454545454545455}\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       color_sum       0.48      0.68      0.57        22\n",
            "      threat_sum       0.00      0.00      0.00         3\n",
            "      fading_sum       0.00      0.00      0.00         6\n",
            "        form_sum       0.59      0.94      0.72        18\n",
            "kinesthetics_sum       1.00      0.21      0.35        19\n",
            "\n",
            "       micro avg       0.56      0.53      0.55        68\n",
            "       macro avg       0.41      0.37      0.33        68\n",
            "    weighted avg       0.59      0.53      0.47        68\n",
            "     samples avg       0.55      0.60      0.53        68\n",
            "\n",
            "############# Epoch 2  Done   #############\n",
            "\n",
            "############# Epoch 3: Training Start   #############\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "############# Epoch 3: Training End     #############\n",
            "############# Epoch 3: Validation Start   #############\n",
            "tensor([[0.3299, 0.1385, 0.1978, 0.3393, 0.3833],\n",
            "        [0.3694, 0.1331, 0.2162, 0.3235, 0.3486],\n",
            "        [0.3215, 0.1096, 0.1581, 0.3596, 0.3223],\n",
            "        [0.3468, 0.1287, 0.2037, 0.3422, 0.3163],\n",
            "        [0.3049, 0.0995, 0.1416, 0.5068, 0.2810],\n",
            "        [0.3325, 0.1086, 0.1501, 0.5514, 0.2847],\n",
            "        [0.3836, 0.1155, 0.1634, 0.3091, 0.3002],\n",
            "        [0.3330, 0.1001, 0.1506, 0.5559, 0.2558],\n",
            "        [0.3534, 0.1345, 0.2175, 0.2969, 0.3454],\n",
            "        [0.3147, 0.1007, 0.1383, 0.4150, 0.2889],\n",
            "        [0.3441, 0.0942, 0.1287, 0.4739, 0.2604],\n",
            "        [0.4282, 0.1132, 0.1959, 0.2982, 0.2924],\n",
            "        [0.2803, 0.1114, 0.1498, 0.5189, 0.2907],\n",
            "        [0.3238, 0.1105, 0.1550, 0.5626, 0.3011],\n",
            "        [0.3835, 0.1137, 0.1699, 0.3809, 0.2722],\n",
            "        [0.2963, 0.1071, 0.1587, 0.4778, 0.3376]], device='cuda:0')\n",
            "tensor([[0.3478, 0.1076, 0.1330, 0.3967, 0.2869],\n",
            "        [0.3474, 0.1202, 0.1889, 0.3260, 0.3188],\n",
            "        [0.3956, 0.0880, 0.1222, 0.3829, 0.2486],\n",
            "        [0.3021, 0.1547, 0.1937, 0.3782, 0.3810],\n",
            "        [0.3070, 0.1158, 0.1454, 0.4302, 0.3234],\n",
            "        [0.4832, 0.1255, 0.2080, 0.2804, 0.3009],\n",
            "        [0.3628, 0.1107, 0.1638, 0.3009, 0.2874],\n",
            "        [0.5523, 0.1480, 0.2536, 0.2873, 0.2905],\n",
            "        [0.3371, 0.1028, 0.1487, 0.4558, 0.2908],\n",
            "        [0.3238, 0.1059, 0.1571, 0.4424, 0.2892],\n",
            "        [0.5276, 0.1619, 0.2630, 0.2771, 0.3112],\n",
            "        [0.3170, 0.1153, 0.1627, 0.4391, 0.3197],\n",
            "        [0.4000, 0.1060, 0.1544, 0.5773, 0.2894],\n",
            "        [0.3750, 0.1191, 0.1733, 0.3715, 0.3013],\n",
            "        [0.3238, 0.1133, 0.1687, 0.5737, 0.2983],\n",
            "        [0.4718, 0.1289, 0.1950, 0.2767, 0.3101]], device='cuda:0')\n",
            "tensor([[0.3861, 0.0919, 0.1418, 0.4946, 0.2248],\n",
            "        [0.3667, 0.1377, 0.2066, 0.3098, 0.3604],\n",
            "        [0.3353, 0.1157, 0.1675, 0.3414, 0.3417],\n",
            "        [0.3408, 0.1444, 0.1678, 0.3526, 0.3689],\n",
            "        [0.2919, 0.1331, 0.1760, 0.3978, 0.3907],\n",
            "        [0.2991, 0.1113, 0.1404, 0.4102, 0.3184],\n",
            "        [0.3315, 0.1393, 0.1961, 0.3532, 0.3544],\n",
            "        [0.3131, 0.1252, 0.1552, 0.3627, 0.3289],\n",
            "        [0.2751, 0.1168, 0.1529, 0.4941, 0.2922],\n",
            "        [0.3143, 0.1213, 0.1743, 0.3608, 0.3502],\n",
            "        [0.2583, 0.1371, 0.1810, 0.4437, 0.3501],\n",
            "        [0.3238, 0.1234, 0.1805, 0.5829, 0.2870],\n",
            "        [0.3069, 0.1118, 0.1681, 0.4298, 0.3179],\n",
            "        [0.4120, 0.1465, 0.2244, 0.3214, 0.3311],\n",
            "        [0.3060, 0.1147, 0.1514, 0.4874, 0.3361],\n",
            "        [0.2816, 0.1224, 0.1556, 0.4698, 0.2993]], device='cuda:0')\n",
            "tensor([[0.3420, 0.1388, 0.1925, 0.6414, 0.2875],\n",
            "        [0.3748, 0.1153, 0.1653, 0.3115, 0.3149],\n",
            "        [0.4098, 0.1100, 0.1718, 0.3196, 0.3030]], device='cuda:0')\n",
            "############# Epoch 3: Validation End     #############\n",
            "Epoch: 3 \tAvgerage Training Loss: 0.025509 \tAverage Validation Loss: 0.116026\n",
            "############# Epoch 3  Metrics   #############\n",
            "\n",
            "\n",
            "{'roc_auc': 0.7165775401069518, 'hamming_loss': 0.19607843137254902, 'f1': 0.5901639344262295}\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       color_sum       0.82      0.64      0.72        22\n",
            "      threat_sum       0.00      0.00      0.00         3\n",
            "      fading_sum       0.00      0.00      0.00         6\n",
            "        form_sum       0.53      0.94      0.68        18\n",
            "kinesthetics_sum       1.00      0.26      0.42        19\n",
            "\n",
            "       micro avg       0.67      0.53      0.59        68\n",
            "       macro avg       0.47      0.37      0.36        68\n",
            "    weighted avg       0.69      0.53      0.53        68\n",
            "     samples avg       0.63      0.60      0.59        68\n",
            "\n",
            "############# Epoch 3  Done   #############\n",
            "\n",
            "############# Epoch 4: Training Start   #############\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "############# Epoch 4: Training End     #############\n",
            "############# Epoch 4: Validation Start   #############\n",
            "tensor([[0.3173, 0.1196, 0.1729, 0.3259, 0.3810],\n",
            "        [0.4107, 0.1163, 0.2089, 0.2839, 0.3345],\n",
            "        [0.3120, 0.0786, 0.1183, 0.3726, 0.2738],\n",
            "        [0.4064, 0.1191, 0.2116, 0.2943, 0.3052],\n",
            "        [0.2865, 0.0812, 0.1211, 0.4807, 0.2430],\n",
            "        [0.3928, 0.0996, 0.1385, 0.6459, 0.2365],\n",
            "        [0.4519, 0.1006, 0.1548, 0.2794, 0.2842],\n",
            "        [0.3489, 0.0792, 0.1275, 0.5804, 0.1984],\n",
            "        [0.3900, 0.1172, 0.2094, 0.2546, 0.3320],\n",
            "        [0.3557, 0.0836, 0.1225, 0.3777, 0.2535],\n",
            "        [0.4416, 0.0883, 0.1223, 0.5632, 0.1989],\n",
            "        [0.5024, 0.1015, 0.1946, 0.2748, 0.2769],\n",
            "        [0.2833, 0.0838, 0.1175, 0.5214, 0.2266],\n",
            "        [0.3519, 0.0953, 0.1391, 0.6454, 0.2461],\n",
            "        [0.4669, 0.0947, 0.1557, 0.3693, 0.2276],\n",
            "        [0.3066, 0.0765, 0.1285, 0.5641, 0.2686]], device='cuda:0')\n",
            "tensor([[0.4127, 0.0786, 0.1072, 0.4412, 0.2161],\n",
            "        [0.3961, 0.0980, 0.1732, 0.2871, 0.2899],\n",
            "        [0.4737, 0.0774, 0.1123, 0.4175, 0.2130],\n",
            "        [0.2998, 0.1401, 0.1811, 0.3516, 0.3766],\n",
            "        [0.3146, 0.0829, 0.1068, 0.4304, 0.2656],\n",
            "        [0.5687, 0.1356, 0.2384, 0.2607, 0.3166],\n",
            "        [0.4028, 0.0849, 0.1384, 0.2817, 0.2528],\n",
            "        [0.6361, 0.1837, 0.3085, 0.2897, 0.3090],\n",
            "        [0.3845, 0.0744, 0.1194, 0.5594, 0.2116],\n",
            "        [0.4129, 0.0902, 0.1583, 0.3889, 0.2398],\n",
            "        [0.5922, 0.1861, 0.2990, 0.2632, 0.3308],\n",
            "        [0.3549, 0.0823, 0.1240, 0.5400, 0.2292],\n",
            "        [0.4855, 0.1279, 0.1413, 0.5835, 0.2228],\n",
            "        [0.4924, 0.1068, 0.1777, 0.3829, 0.2649],\n",
            "        [0.3860, 0.1038, 0.1534, 0.6670, 0.2464],\n",
            "        [0.5329, 0.1244, 0.1958, 0.2622, 0.3094]], device='cuda:0')\n",
            "tensor([[0.4543, 0.0900, 0.1399, 0.4819, 0.2042],\n",
            "        [0.4173, 0.1175, 0.1973, 0.2707, 0.3422],\n",
            "        [0.3639, 0.0882, 0.1450, 0.3134, 0.2980],\n",
            "        [0.3351, 0.1269, 0.1443, 0.3383, 0.3656],\n",
            "        [0.2624, 0.1161, 0.1527, 0.3878, 0.3925],\n",
            "        [0.3158, 0.0766, 0.1032, 0.4483, 0.2458],\n",
            "        [0.3509, 0.1220, 0.1803, 0.3198, 0.3359],\n",
            "        [0.3249, 0.0992, 0.1269, 0.3384, 0.2841],\n",
            "        [0.2615, 0.0968, 0.1280, 0.4475, 0.2578],\n",
            "        [0.3163, 0.1023, 0.1595, 0.3257, 0.3369],\n",
            "        [0.2483, 0.1300, 0.1699, 0.3889, 0.3539],\n",
            "        [0.2974, 0.0951, 0.1348, 0.5702, 0.2365],\n",
            "        [0.3114, 0.0759, 0.1272, 0.5092, 0.2403],\n",
            "        [0.4896, 0.1497, 0.2362, 0.2916, 0.3391],\n",
            "        [0.3024, 0.0910, 0.1223, 0.5702, 0.2760],\n",
            "        [0.2780, 0.1045, 0.1327, 0.4296, 0.2669]], device='cuda:0')\n",
            "tensor([[0.3278, 0.1119, 0.1574, 0.6606, 0.2231],\n",
            "        [0.4342, 0.0926, 0.1450, 0.2866, 0.2766],\n",
            "        [0.5155, 0.0922, 0.1736, 0.3067, 0.2822]], device='cuda:0')\n",
            "############# Epoch 4: Validation End     #############\n",
            "Epoch: 4 \tAvgerage Training Loss: 0.024166 \tAverage Validation Loss: 0.109774\n",
            "############# Epoch 4  Metrics   #############\n",
            "\n",
            "\n",
            "{'roc_auc': 0.7279411764705882, 'hamming_loss': 0.2, 'f1': 0.6046511627906976}\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       color_sum       0.64      0.82      0.72        22\n",
            "      threat_sum       0.00      0.00      0.00         3\n",
            "      fading_sum       0.00      0.00      0.00         6\n",
            "        form_sum       0.59      0.94      0.72        18\n",
            "kinesthetics_sum       1.00      0.21      0.35        19\n",
            "\n",
            "       micro avg       0.64      0.57      0.60        68\n",
            "       macro avg       0.45      0.39      0.36        68\n",
            "    weighted avg       0.64      0.57      0.52        68\n",
            "     samples avg       0.64      0.65      0.60        68\n",
            "\n",
            "############# Epoch 4  Done   #############\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "trained_model, val_targets_study, val_outputs_study = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path, df_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFJgtECyBvt5"
      },
      "source": [
        "TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "ti3K0jiLBox7"
      },
      "outputs": [],
      "source": [
        "test_dataset = CustomDataset(df_test, tokenizer, MAX_LEN)\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWePtx-juAM7",
        "outputId": "1fc70e7b-25b7-46dd-f94a-8260dae5dde7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'roc_auc': 0.6171428571428571, 'hamming_loss': 0.2894736842105263, 'f1': 0.43298969072164945}\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "       color_sum       0.42      0.73      0.54        15\n",
            "      threat_sum       0.00      0.00      0.00         6\n",
            "      fading_sum       0.00      0.00      0.00         3\n",
            "        form_sum       0.45      0.69      0.55        13\n",
            "kinesthetics_sum       1.00      0.08      0.14        13\n",
            "\n",
            "       micro avg       0.45      0.42      0.43        50\n",
            "       macro avg       0.37      0.30      0.24        50\n",
            "    weighted avg       0.50      0.42      0.34        50\n",
            "     samples avg       0.43      0.49      0.43        50\n",
            "\n",
            "EVAL METRICS: {'roc_auc': 0.6171428571428571, 'hamming_loss': 0.2894736842105263, 'f1': 0.43298969072164945}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "metrics, val_outputs_study, val_targets_study =  compute_test_metrics(test_data_loader, val_outputs_study, val_targets_study)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyXe4BHl-aj0"
      },
      "source": [
        "Saving the study probabilities and targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "r263p-PTFQe6"
      },
      "outputs": [],
      "source": [
        "val_outputs_study = [value for row_proba in val_outputs_study for value in row_proba]\n",
        "val_outputs_study = [value for row_proba in val_outputs_study for value in row_proba]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "GwBzIaQWF0vB"
      },
      "outputs": [],
      "source": [
        "val_targets_study = [value for row_proba in val_targets_study for value in row_proba]\n",
        "val_targets_study = [value for row_proba in val_targets_study for value in row_proba]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "jT-TYZYg-dJ6"
      },
      "outputs": [],
      "source": [
        "df_prob_target = pd.DataFrame([val_outputs_study, val_targets_study]).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "My2BGj_uCpVf"
      },
      "outputs": [],
      "source": [
        "df_prob_target.rename(columns = {0:\"Probability\", 1:\"Actual_target\"}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "0XmVotJlGbQ9"
      },
      "outputs": [],
      "source": [
        "df_prob_target.to_csv(\"prob_target_study.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
