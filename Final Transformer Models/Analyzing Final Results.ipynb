{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Analyzing Final Results.ipynb',\n",
       " 'FInal models.ipynb',\n",
       " 'metrics_final_individual_content.json',\n",
       " 'metrics_final_individual_determinant.json',\n",
       " 'metrics_final_macro_content.json',\n",
       " 'metrics_final_macro_determinants.json',\n",
       " 'nancy_contents_individual_labels_eng.csv',\n",
       " 'nancy_contents_macro_labels_english.csv',\n",
       " 'nancy_determinants_individual_labels_eng.csv',\n",
       " 'nancy_determinants_macro_labels_english.csv',\n",
       " 'TO-DO.txt']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = ['metrics_final_individual_content.json',\n",
    " 'metrics_final_individual_determinant.json',\n",
    " 'metrics_final_macro_content.json',\n",
    " 'metrics_final_macro_determinants.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"metrics_final_individual_content.json\", \"r\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# validation_metrics = data[0][\"metrics\"][\"best_metrics\"]\n",
    "# validation_metrics.keys()\n",
    "\n",
    "# del validation_metrics[\"proba_outputs\"]\n",
    "# del validation_metrics[\"actual_classes\"]\n",
    "\n",
    "# test = data[0][\"metrics\"][\"test\"]\n",
    "# test.keys()\n",
    "\n",
    "# del test[\"test_proba\"]\n",
    "# del test[\"test_targets\"]\n",
    "\n",
    "# df  = pd.DataFrame(test, index=[0])\n",
    "# list(data[0][\"metrics\"].keys())[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(data[0][\"metrics\"].keys())[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0][\"metrics\"][\"epoch_31\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(data[0][\"metrics\"][\"epoch_5\"].keys())[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0][\"metrics\"][\"test\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['metrics_final_individual_content.json',\n",
       " 'metrics_final_individual_determinant.json',\n",
       " 'metrics_final_macro_content.json',\n",
       " 'metrics_final_macro_determinants.json']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"metrics_final_individual_determinant.json\", \"r\") as f:\n",
    "#     data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0][\"metrics\"][\"test\"][\"test_proba\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics_final_individual_content\n",
      "['(A)', '(AD)', '(H)', '(HD)', 'A', 'ABS', 'AD', 'ALIM', 'ANAT', 'ARCH', 'ART', 'BOT', 'ELEM', 'FRAG', 'GÉO', 'H', 'HD', 'MQ', 'NAT', 'OBJ', 'PAYS', 'RADIO', 'SC', 'SCÈNE', 'SEX', 'SG', 'VÊT']\n",
      "27\n",
      "metrics_final_individual_determinant\n",
      "['C', \"C'\", \"C'F\", 'CF', \"CF'\", 'CLOB', 'CLOBF', 'E', 'EF', 'F', 'FC', \"FC'\", 'FCLOB', 'FE', 'K', 'KAN', 'KOB', 'KP']\n",
      "18\n",
      "metrics_final_macro_content\n",
      "['animal_sum', 'human_sum', 'abs_sum', 'food_sum', 'art_arch_sum', 'nature_sum', 'fragment_sum', 'geo_sum', 'object_sum', 'science_sum', 'graphic_sum']\n",
      "11\n",
      "metrics_final_macro_determinants\n",
      "['color_sum', 'threat_sum', 'fading_sum', 'form_sum', 'kinesthetics_sum']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "results_test = pd.DataFrame()\n",
    "results_validation = pd.DataFrame()\n",
    "\n",
    "for model in final_results:\n",
    "    model_name = model.split(\".json\")[0]\n",
    "    with open(model, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    last_epoch = list(data[0][\"metrics\"].keys())[-3]\n",
    "    validation_metrics = data[0][\"metrics\"][last_epoch]\n",
    "    params = data[0][\"hyperparameters\"]\n",
    "    # print(list(data[0][\"metrics\"].keys()))\n",
    "    \n",
    "    y_true_test = data[0][\"metrics\"][\"test\"][\"test_targets\"]\n",
    "    prob_test = data[0][\"metrics\"][\"test\"][\"test_proba\"]\n",
    "    \n",
    "    test_metrics = data[0][\"metrics\"][\"test\"]\n",
    "    \n",
    "    list_keys = list(validation_metrics.keys())[-2:]\n",
    "    \n",
    "    argmaxed_predictions = []\n",
    "    \n",
    "    for keys in list(validation_metrics.keys()):\n",
    "        if keys in list_keys[-2]:\n",
    "            below_threshold = validation_metrics[keys]\n",
    "            for pred in below_threshold:\n",
    "                for pre in pred:\n",
    "                    # print(pre)\n",
    "                    pre_array = np.array(pre)\n",
    "                    y_pred = np.zeros(pre_array.shape)\n",
    "                    y_pred[np.where(pre_array>=0.5)] = 1\n",
    "                    # print(y_pred)\n",
    "                    \n",
    "                    filter_ = [1 for element in pre if element > 0.5]\n",
    "                    positive_preds = [pre.index(element) for element in pre if element > 0.5]\n",
    "                    # print(filter_)\n",
    "                    if 1 not in filter_:\n",
    "                        idx = np.argmax(pre)\n",
    "                        best_pred = pre[idx]\n",
    "                        # print(best_pred)\n",
    "                        y_pred[idx] = 1\n",
    "                        \n",
    "                    argmaxed_predictions.append(y_pred)\n",
    "                    \n",
    "            del validation_metrics[keys]\n",
    "        elif keys in list_keys[-1]:\n",
    "            # print(validation_metrics[keys])\n",
    "            y_true = validation_metrics[keys]\n",
    "            # del validation_metrics[keys]\n",
    "            \n",
    "    argmaxed_predictions = np.array(argmaxed_predictions).astype(np.float32)\n",
    "    y_true = np.array(y_true).astype(np.float32)            \n",
    "    \n",
    "    # now i have the argmaxed preds and the true preds\n",
    "    \n",
    "    f1 = f1_score(y_true, argmaxed_predictions, average=\"micro\")\n",
    "    acc = accuracy_score(y_true, argmaxed_predictions)\n",
    "    precision = precision_score(y_true, argmaxed_predictions, average=\"micro\")\n",
    "    recall = recall_score(y_true, argmaxed_predictions, average=\"micro\")\n",
    "    # report =classification_report(y_true, argmaxed_predictions, labels=data[0][\"metrics\"][\"labels\"])\n",
    "    hamming = hamming_loss(y_true, argmaxed_predictions)\n",
    "\n",
    "    # print(report)\n",
    "\n",
    "    for key in list(validation_metrics.keys()):\n",
    "        new_key_name = key.split(\"_epoch\")[0]\n",
    "        validation_metrics[new_key_name] = validation_metrics[key]\n",
    "        # del validation_metrics[key]\n",
    "    \n",
    "    val_arg = {}\n",
    "    val_arg[\"model\"] = model_name\n",
    "    val_arg[\"f1_arg\"] = f1\n",
    "    val_arg[\"acc_arg\"] = acc\n",
    "    val_arg[\"precision_arg\"] = precision\n",
    "    val_arg[\"recall_arg\"] = recall\n",
    "    \n",
    "    \n",
    "    \n",
    "    test_metrics[\"model\"] = model_name\n",
    "    # print(test_metrics.keys())\n",
    "    \n",
    "    # prob_test = data[0][\"metrics\"][\"test\"][\"test_proba\"]\n",
    "    # print(prob_test)\n",
    "    # y_true_test = data[0][\"metrics\"][\"test\"][\"test_targets\"]\n",
    "    \n",
    "    argmaxed_predictions_test = []\n",
    "    for pred in prob_test:\n",
    "                for pre in pred:\n",
    "                    # print(pre)\n",
    "                    pre_array = np.array(pre)\n",
    "                    y_pred_test = np.zeros(pre_array.shape)\n",
    "                    y_pred_test[np.where(pre_array>=0.5)] = 1\n",
    "                    #print(y_pred)\n",
    "                    \n",
    "                    filter_ = [1 for element in pre if element > 0.5]\n",
    "                    positive_preds = [pre.index(element) for element in pre if element > 0.5]\n",
    "                    # print(filter_)\n",
    "                    if 1 not in filter_:\n",
    "                        idx = np.argmax(pre)\n",
    "                        best_pred = pre[idx]\n",
    "                        # print(best_pred)\n",
    "                        # print(y_pred)\n",
    "                        y_pred_test[idx] = 1\n",
    "                        \n",
    "                    argmaxed_predictions_test.append(y_pred_test)\n",
    "    \n",
    "    argmaxed_predictions_test = np.array(argmaxed_predictions_test)\n",
    "    y_true_test = np.array(y_true_test)\n",
    "    \n",
    "    f1_test = f1_score(y_true_test, argmaxed_predictions_test, average=\"micro\")\n",
    "    acc_test = accuracy_score(y_true_test, argmaxed_predictions_test)\n",
    "    precision_test = precision_score(y_true_test, argmaxed_predictions_test, average=\"micro\")\n",
    "    recall_test = recall_score(y_true_test, argmaxed_predictions_test, average=\"micro\")\n",
    "    hamming_test = hamming_loss(y_true_test, argmaxed_predictions_test)\n",
    "    # report_test = classification_report(y_true_test, argmaxed_predictions_test, labels=data[0][\"metrics\"][\"labels\"])\n",
    "    # print(report_test)\n",
    "    \n",
    "    test_arg = {}\n",
    "    test_arg[\"model\"] = model_name\n",
    "    test_arg[\"f1_arg\"] = f1_test\n",
    "    test_arg[\"acc_arg\"] = acc_test\n",
    "    test_arg[\"precision_arg\"] = precision_test\n",
    "    test_arg[\"recall_arg\"] = recall_test\n",
    "    test_arg[\"hamming\"] = hamming_test\n",
    "    # del test_metrics[\"test_proba\"]\n",
    "    # del test_metrics[\"test_targets\"]\n",
    "\n",
    "\n",
    "    df_val = pd.DataFrame(val_arg, index=[0])\n",
    "    df_test = pd.DataFrame(test_arg, index=[0])\n",
    "    df_params = pd.DataFrame(params, index=[0])\n",
    "\n",
    "    results_test = pd.concat([results_test, df_test], axis=0)\n",
    "    results_validation = pd.concat([results_validation, df_val], axis=0)\n",
    "    \n",
    "    print(model_name)\n",
    "    print(data[0][\"metrics\"][\"labels\"])\n",
    "    print(len(data[0][\"metrics\"][\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1_arg</th>\n",
       "      <th>acc_arg</th>\n",
       "      <th>precision_arg</th>\n",
       "      <th>recall_arg</th>\n",
       "      <th>hamming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metrics_final_individual_content</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.014493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metrics_final_individual_determinant</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.063889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metrics_final_macro_content</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.020661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metrics_final_macro_determinants</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  model    f1_arg   acc_arg  precision_arg  \\\n",
       "0      metrics_final_individual_content  0.816327  0.739130       0.800000   \n",
       "0  metrics_final_individual_determinant  0.465116  0.400000       0.500000   \n",
       "0           metrics_final_macro_content  0.888889  0.863636       0.909091   \n",
       "0      metrics_final_macro_determinants  0.590909  0.444444       0.619048   \n",
       "\n",
       "   recall_arg   hamming  \n",
       "0    0.833333  0.014493  \n",
       "0    0.434783  0.063889  \n",
       "0    0.869565  0.020661  \n",
       "0    0.565217  0.200000  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrrr}\n",
      "\\toprule\n",
      " & model & f1_arg & acc_arg & precision_arg & recall_arg & hamming \\\\\n",
      "\\midrule\n",
      "0 & metrics_final_individual_content & 0.816327 & 0.739130 & 0.800000 & 0.833333 & 0.014493 \\\\\n",
      "0 & metrics_final_individual_determinant & 0.465116 & 0.400000 & 0.500000 & 0.434783 & 0.063889 \\\\\n",
      "0 & metrics_final_macro_content & 0.888889 & 0.863636 & 0.909091 & 0.869565 & 0.020661 \\\\\n",
      "0 & metrics_final_macro_determinants & 0.590909 & 0.444444 & 0.619048 & 0.565217 & 0.200000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results_test.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(results_test.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
